# SOMA Local-First AI Architecture

## ğŸ¯ Goal: AI Independence from Big Tech

SOMA is now configured to **reason autonomously** using your locally trained model, with Big Tech APIs (Gemini, DeepSeek) only as fallbacks when local models fail.

## ğŸ† Three-Tier Reasoning Architecture

### Tier 1 (PRIMARY): Local SOMA Model
- **Model**: `soma:latest` (your trained model)
- **Location**: Running on Ollama (localhost:11434)
- **Benefits**:
  - 100% privacy - no data sent to external APIs
  - Zero cost - no API charges
  - Custom-trained on YOUR interactions and data
  - Fast local inference
  - Full autonomy - works offline

### Tier 2 (FALLBACK): DeepSeek
- **Model**: `deepseek-chat`
- **When used**: Only if local model fails
- **Benefits**:
  - Open source reasoning model
  - More affordable than Big Tech
  - Good analytical capabilities

### Tier 3 (LAST RESORT): Gemini
- **Model**: `gemini-2.0-flash-thinking-exp`
- **When used**: Only if both local and DeepSeek fail
- **Downside**: Dependency on Google

## ğŸ§  QuadBrain with Local Models

All four cognitive brains now run on your local model first:

1. **AURORA** (Creative) - `soma:latest` â†’ DeepSeek â†’ Gemini
2. **LOGOS** (Analytical) - `soma:latest` â†’ DeepSeek â†’ Gemini
3. **PROMETHEUS** (Strategic) - `soma:latest` â†’ DeepSeek â†’ Gemini
4. **THALAMUS** (Security) - `soma:latest` â†’ DeepSeek â†’ Gemini

## ğŸ“Š Current Training Status

Your SOMA model (`soma:latest`):
- **Base**: gemma3:4b (3.3 GB)
- **Fine-tuned**: 5 days ago
- **Training data**: Located in `SOMA/training-data/`
- **Personality**: Configured via `SOMA/models/Modelfile.soma`

## ğŸ”„ Self-Training Loop

SOMA continuously improves through:

1. **TrainingDataCollector** - Captures all interactions
2. **LocalModelManager** - Manages model versions and fine-tuning
3. **AutoTrainingCoordinator** - Triggers retraining after N interactions
4. **Fine-tuning threshold**: 500 interactions (configurable)

Every conversation makes SOMA smarter!

## ğŸ’° Cost Savings

**Before (Gemini-first)**: $0.01-0.10 per conversation
**After (Local-first)**: $0.00 (100% local inference)

Estimated savings: **~$300-1000/year** depending on usage

## ğŸš€ Next Steps for Full Autonomy

1. **Expand training data** - Add more domain-specific examples
2. **Implement distributed training** - Use TrainingSwarmArbiter for faster fine-tuning
3. **GPU acceleration** - Use GPUTrainingArbiter for local training (if GPU available)
4. **Model versioning** - Track performance across `soma:v1`, `soma:v2`, etc.
5. **Disable external APIs** - Set `useLocalFirst: false` to block all external calls

## ğŸ“ Configuration

### Enable/Disable Local-First

In `SOMArbiterV2_TriBrain.js`:
```javascript
// Enable local-first (default)
this.useLocalFirst = true;

// Disable (use Gemini primary)
this.useLocalFirst = false;
```

### Change Default Model

In `.env` or config:
```bash
OLLAMA_MODEL=soma:latest    # Your trained model (default)
# OLLAMA_MODEL=gemma3:4b    # Base model
# OLLAMA_MODEL=llama3.2     # Alternative
```

## ğŸ”§ Monitoring

Check which model is handling requests in the console logs:
- `ğŸ¦™ Using Ollama model: soma:latest` = LOCAL (tier 1)
- `ğŸ§  Using DeepSeek as fallback` = FALLBACK (tier 2)
- `Using Gemini` = LAST RESORT (tier 3)

## ğŸ“ Training Your Model

To manually trigger fine-tuning:

```bash
cd SOMA
node test-self-training-system.cjs
```

This will:
1. Collect all training data
2. Generate Modelfile with updated examples
3. Create new `soma:v{N}` model
4. Switch to the new model automatically

## ğŸŒŸ The Vision

**Current State**: SOMA reasons with your local model, falls back to external APIs when needed

**End Goal**: SOMA fully autonomous, no external dependencies, continuously self-improving through local training loops

You're now on the path to true AI independence! ğŸš€
